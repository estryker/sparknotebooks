{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark examples using Airport Data\n",
    "I started with this, but it doesn't exist anymore. [This intro](http://sparktutorials.net/analyzing-flight-data:-a-gentle-introduction-to-graphx-in-spark)\n",
    "\n",
    "For more info on the dataset, see: [This page](http://stat-computing.org/dataexpo/2009/the-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "Read the data into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_1 = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"/Users/Ethan/notebooks/2008.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many items are in the dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are all the Column names in the Dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select out the Origin and Dest columns and show the top 20 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.select(col(\"Origin\"), col(\"Dest\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save off the unique airport codes into their own Dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airport_codes = df_1.select(col(\"Origin\")).distinct.union(df_1.select(col(\"Dest\"))).distinct\n",
    "airport_codes.cache\n",
    "airport_codes.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a GraphX graph. make some nodes with properties\n",
    "The vertices will have the form (id:Long,airportCode:String)\n",
    "\n",
    "However, we aren't going to use a Hash function to create the ID's, so we will make to lookup Maps along the way:\n",
    "\n",
    "idToCode and codeToId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val vertices = airport_codes.rdd.map(r => r.getString(0)).zipWithUniqueId.map(_.swap)\n",
    "vertices.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tmp = vertices.collect\n",
    "val idToCode = Map(tmp:_*)\n",
    "idToCode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val codeToId = Map(tmp.map(_.swap) :_*)\n",
    "codeToId(\"BGM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can create our Edges from the data\n",
    "Each Edge will connect the ID of each airport code (which is why we have the codeToId Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val flightsFromTo = df_1.select(col(\"Origin\"),col(\"Dest\"))\n",
    "flightsFromTo.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our graph\n",
    "*Note that we only want to analyze the distinct edges*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val edges = flightsFromTo.rdd.distinct.map(r => Edge(codeToId(r.getString(0)),codeToId(r.getString(1)), \"route\"))\n",
    "val graph = Graph(vertices,edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many vertices and edges does the graph have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.numVertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.numEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PageRank to find the most important airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ranks = graph.pageRank(0.001).vertices\n",
    "ranks.cache\n",
    "ranks.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks.sortBy(r => r._2, false).take(1b0).foreach(r => println(idToCode(r._1) + \"\\t\" + r._2.toString))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that all of the airports are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val components = graph.connectedComponents\n",
    "components.cache\n",
    "components.vertices.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components.vertices.map(r => r._2).distinct.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Do some standard dataframe calculations\n",
    "1. what is the most common origin? \n",
    "1. most common dest?\n",
    "1. what is the most common route?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.groupBy(\"Origin\").agg(count(\"*\") as \"times_origin\").orderBy(desc(\"times_origin\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1.groupBy(\"Dest\").agg(count(\"*\") as \"times_dest\").orderBy(desc(\"times_dest\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1.groupBy(\"Origin\", \"Dest\").agg(count(\"*\") as \"route_count\").orderBy(desc(\"route_count\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the average carrier delay throughout the day\n",
    "For now lets aggregate by hour and by airport. This requires standard DataFrame functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_2 = df_1.withColumn(\"depHour\", col(\"depTime\").substr(0,2))\n",
    "df_2.select(\"depTime\", \"depHour\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oops - we need leading 0's on our times\n",
    "Lets fix our depTime Column first. Write a Scala function called fixTime that formats a String to make sure it has 4 digits with a leading 0 if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixTime(timeStr: String) :String = {\n",
    "   \"%04d\".format(timeStr.toInt)\n",
    "}\n",
    "fixTime(\"617\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the fixTime function into a User Defined function (udf)\n",
    "*Note: you need to import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "val fixTimeUdf = udf(fixTime(_:String))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new Dataframe and fix the depTime Column using the fixTimeUdf that was just created\n",
    "* replacing the old column will require creating a new one, dropping the old, then renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val df_3 = df_1.withColumn(\"fixedDepTime\",fixTimeUdf(col(\"depTime\"))).drop(\"depTime\").withColumnRenamed(\"fixedDepTime\",\"depTime\")\n",
    "df_3.select(\"depTime\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new Column called depHour that just contains the hour from depTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_4 = df_3.withColumn(\"depHour\",col(\"depTime\").substr(0,2))\n",
    "df_4.select(\"Origin\",\"depHour\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataframe that contains the total delay from the DepDelay column, grouping by Origin and depHour\n",
    "*Use the .cast Column operator to convert DepDelay to an Int.*\n",
    "\n",
    "*This will fail - can you figure out why?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val total_delays = df_4.groupBy(\"Origin\",\"depHour\").agg(sum(col(\"DepDelay\").cast(\"Int\")) as \"total_delay\")\n",
    "total_delays.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Scala method that converts a String to an Integer called failSafeToInt that returns 0 if the String is not a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.matching.Regex._\n",
    "def failSafeToInt(input: String): Int = {\n",
    "val num = \"\"\"^\\d+$\"\"\".r\n",
    "   input match {\n",
    "      case num() => input.toInt\n",
    "      case _ => 0\n",
    "   }\n",
    "}\n",
    "failSafeToInt(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failSafeToInt(\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I don't know why this doesn't work (bonus points for fixing?) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val toIntUdf = udf(failSafeToInt(_:String))\n",
    "val total_delays = df_4.groupBy(\"Origin\",\"depHour\").agg(sum(toIntUdf(col(\"DepDelay\"))) as \"total_delay\")\n",
    "total_delays.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So ... create a Dataframe after filtering out the N/A's in the DepDelay column and calculate totals from that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val total_delays = df_4.filter(col(\"DepDelay\") !== \"NA\").groupBy(\"Origin\",\"depHour\").agg(sum(toIntUdf(col(\"DepDelay\"))) as \"total_delay\")\n",
    "total_delays.orderBy(\"Origin\",\"depHour\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for some Windowing functions\n",
    "1. Find the top 2 worst carriers in terms of delays at each airport\n",
    "1. Calculate the difference between the top delayed airline at each airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by creating a Dataframe that contains the delays of each uniqueCarrier at each origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val carrier_delays = df_4.groupBy(\"Origin\",\"uniqueCarrier\").agg(sum(\"CarrierDelay\") as \"totalCarrierDelay\")\n",
    "carrier_delays.orderBy(asc(\"Origin\"), desc(\"totalCarrierDelay\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the worst two carriers at each airport\n",
    "The window specifies how we partition the data when later ranking. In our case, we partition by Origin. \n",
    "\n",
    "The window also specifies the ordering so that the rank function knows how to do its job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "val windowSpec = Window.partitionBy(\"Origin\").orderBy(desc(\"totalCarrierDelay\"))\n",
    "val ranked_df = carrier_delays.withColumn(\"rank\", dense_rank().over(windowSpec) as \"rank\").filter(col(\"rank\") <= 2)\n",
    "ranked_df.orderBy(\"Origin\",\"rank\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we use the window to find diffs between the worst and all the rest of the carriers in terms of delays\n",
    "delay_diff is a Column expression built up from calling the max Column function on the carrier_delays dataframe. \n",
    "\n",
    "The .over() call turns the max into a windowing function. \n",
    "\n",
    "So max will operate in the partition specified by the window spec. Again, in our example we partitioned by airport. So the max function finds the max totalCarrierDelay within each partition, and subtracts the totalCarrierDelay for each carrier in that partition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val delay_diff = max(carrier_delays(\"totalCarrierDelay\")).over(windowSpec) - carrier_delays(\"totalCarrierDelay\")\n",
    "val delay_diffs_df = carrier_delays.withColumn(\"delayDiff\", delay_diff)\n",
    "delay_diffs_df.orderBy(\"Origin\",\"delayDiff\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(delay_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for some Dataset functions ...\n",
    "1. create a case class called AirportCarriers that contains Origin and UniqueCarrier. \n",
    "1. create a DataFrame called carrier_df that just have the Origin and UniqueCarrier. \n",
    "1. create a Dataset from carrier_df using the .as[] methos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class AirportCarriers(Origin: String, UniqueCarrier: String)\n",
    "val carrier_df = df_1.dropDuplicates(\"Origin\",\"UniqueCarrier\") // .as[AirportCarriers]\n",
    "carrier_df.show\n",
    "val carrier_ds = carrier_df.as[AirpoirtCarriers]\n",
    "carrier_ds.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus points for fixing the next couple cells ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val x = spark\n",
    "import x.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// Okay, I can't import spark.implicits._. and I can't convert to a Dataset of Tuple(String,String,Double,Double)\n",
    "// I would also like to be able to specify / construct an encoder for a case class made up of primitives\n",
    "// Leaving this here, but don't need to do this now that I have imported spark.implicits._\n",
    "val ds_1 = delay_diffs_df.as[(String,String,Double,Double)](Encoders.tuple(Encoders.STRING, Encoders.STRING, Encoders.DOUBLE, Encoders.DOUBLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset from the delay_diffs Dataframe using the as[AirportCarriers] method and show the top 20 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds_1 = delay_diffs_df.as[AirportCarriers]\n",
    "ds_1.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use groupByKey Dataset function to group ds_1 by the origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1.groupByKey(r => r.Origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the following in the groupByKey and reduceGroups examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class AirportCarrierSets(Origin:String, UniqueCarrierSet: Set[String])\n",
    "ds_1.map(r => AirportCarrierSets(r.Origin, Set(r.UniqueCarrier)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupByKey / reduceGroups\n",
    "\n",
    "if T is the Type of your Dataset rows (AirportCarriers)\n",
    "and K is the Key that you are extracting from your Dataset to aggregate on:\n",
    "\n",
    "groupByKey takes a function [T => K] and maps a Dataset[T] to a keyValueGroupedDataset[K,T]\n",
    "\n",
    "then reduceGroups takes a function [(T,T) => T] and returns a Dataset of type [K,T]\n",
    "\n",
    "In other words, group by your Key, and specify a function that takes two AirportCarriers and returns a single AirportCarrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val ds_2 = ds_1.groupByKey(_.Origin).reduceGroups((a,b) => AirportCarriers(a.Origin, Set(a.UniqueCarrier.split(\" \") :_*).union(Set(b.UniqueCarrier.split(\" \") :_*)).mkString(\" \")))\n",
    "ds_2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val ans = ds_2.map(r => r._2)\n",
    "println(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
